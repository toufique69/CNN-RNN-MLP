{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST handwritten digits classification with CNNs\n",
    "\n",
    "In this notebook, we'll train a convolutional neural network (CNN, ConvNet) to classify MNIST digits using **Tensorflow** (version $\\ge$ 2.0 required) with the **Keras API**.\n",
    "\n",
    "This notebook builds on the MNIST-MLP notebook, so the recommended order is to go through the MNIST-MLP notebook before starting with this one. \n",
    "\n",
    "First, the needed imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "if not os.path.isfile('pml_utils.py'):\n",
    "  !wget https://raw.githubusercontent.com/csc-training/intro-to-dl/master/day1/pml_utils.py\n",
    "from pml_utils import show_failures\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "\n",
    "from packaging.version import Version as LV\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "print('Using Tensorflow version: {}, and Keras version: {}.'.format(tf.__version__, tf.keras.__version__))\n",
    "assert(LV(tf.__version__) >= LV(\"2.0.0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus) > 0:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "    from tensorflow.python.client import device_lib\n",
    "    for d in device_lib.list_local_devices():\n",
    "        if d.device_type == 'GPU':\n",
    "            print('GPU', d.physical_device_desc)\n",
    "else:\n",
    "    print('No GPU, using CPU instead.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "nb_classes = 10\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# one-hot encoding:\n",
    "Y_train = to_categorical(y_train, nb_classes)\n",
    "Y_test = to_categorical(y_test, nb_classes)\n",
    "\n",
    "print()\n",
    "print('MNIST data loaded: train:',len(X_train),'test:',len(X_test))\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('Y_train:', Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have to do a bit of tensor manipulations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "    \n",
    "print('X_train:', X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Now we are ready to create a convolutional model.\n",
    "\n",
    " * The `Conv2D` layer operate on 2D matrices so we input the digit images directly to the model.  \n",
    " * The `MaxPooling2D` layer reduces the spatial dimensions, that is, makes the image smaller.\n",
    " * The `Flatten` layer flattens the 2D matrices into vectors, so we can then switch to  `Dense` layers as in the MLP model. \n",
    "\n",
    "See https://keras.io/layers/convolutional/, https://keras.io/layers/pooling/ for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "x = layers.Conv2D(32, (3, 3),\n",
    "                  padding='valid',\n",
    "                  activation ='relu')(inputs)\n",
    "x = layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(units=128, activation ='relu')(x)\n",
    "\n",
    "outputs = layers.Dense(units=nb_classes,\n",
    "                       activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs,\n",
    "                    name=\"cnn_model\")\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "Now let's train the CNN model.\n",
    "\n",
    "This is a relatively complex model, so training is considerably slower than with MLPs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 5 # one epoch can take tens of seconds without a GPU\n",
    "\n",
    "history = model.fit(X_train, \n",
    "                    Y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=128,\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,3))\n",
    "\n",
    "ax1.plot(history.epoch,history.history['loss'])\n",
    "ax1.set_title('loss')\n",
    "ax1.set_xlabel('epoch')\n",
    "\n",
    "ax2.plot(history.epoch,history.history['accuracy'])\n",
    "ax2.set_title('accuracy')\n",
    "ax2.set_xlabel('epoch');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "With enough training epochs, the test accuracy should exceed 99%.  \n",
    "\n",
    "You can compare your result with the state-of-the art [here](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html).  Even more results can be found [here](http://yann.lecun.com/exdb/mnist/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = model.evaluate(X_test, Y_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take a closer look at the results using the `show_failures()` helper function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first 10 test digits the CNN classified to a wrong class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "\n",
    "show_failures(predictions, y_test, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `show_failures()` to inspect failures in more detail. For example, here are failures in which the true class was \"6\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "show_failures(predictions, y_test, X_test, trueclass=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 1: A more complex CNN model\n",
    "\n",
    "Your task is to try the same problem as above, but with two convolutional layers. The new model should have the following layers in order:\n",
    "\n",
    "- Input layer\n",
    "- Convolutional (`Conv2D`) layer with 32 units and 3x3 kernels, ReLU activation, valid padding\n",
    "- Another identical convolutional layer\n",
    "- Max pooling (`MaxPooling2D` layer with 2x2 pooling size\n",
    "- Dropout with 0.25 rate\n",
    "- Flatten\n",
    "- Dense layer with 128 units\n",
    "- Dropout with 0.5 rate\n",
    "- Dense output layer (same as in the example above)\n",
    "\n",
    "You can consult the Keras documentation at https://keras.io/.\n",
    "\n",
    "The code below is missing the model definition. You can copy any suitable layers from the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the input layer\n",
    "ex1_inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "# First convolutional layer\n",
    "x = layers.Conv2D(32, (3, 3), padding='valid', activation='relu')(ex1_inputs)\n",
    "\n",
    "# Second convolutional layer\n",
    "x = layers.Conv2D(32, (3, 3), padding='valid', activation='relu')(x)\n",
    "\n",
    "# Max pooling layer to reduce spatial dimensions\n",
    "x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Dropout layer to prevent overfitting\n",
    "x = layers.Dropout(0.25)(x)\n",
    "\n",
    "# Flatten the feature maps into a vector\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Dense layer with 128 units and ReLU activation\n",
    "x = layers.Dense(units=128, activation='relu')(x)\n",
    "\n",
    "# Dropout layer to prevent overfitting\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Output layer with softmax activation for classification\n",
    "ex1_outputs = layers.Dense(units=nb_classes, activation='softmax')(x)\n",
    "\n",
    "# Create the model with input and output layers\n",
    "ex1_model = keras.Model(inputs=ex1_inputs, outputs=ex1_outputs, name=\"better_cnn_model\")\n",
    "\n",
    "# Compile the model with loss function, optimizer, and metrics\n",
    "ex1_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the summary of the model architecture\n",
    "print(ex1_model.summary())\n",
    "\n",
    "# Train the model on the training data\n",
    "epochs = 5\n",
    "ex1_history = ex1_model.fit(X_train, Y_train, epochs=epochs, batch_size=128, verbose=2)\n",
    "\n",
    "# Plot the training progress (loss and accuracy)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "ax1.plot(ex1_history.epoch, ex1_history.history['loss'])\n",
    "ax1.set_title('loss')\n",
    "ax1.set_xlabel('epoch')\n",
    "\n",
    "ax2.plot(ex1_history.epoch, ex1_history.history['accuracy'])\n",
    "ax2.set_title('accuracy')\n",
    "ax2.set_xlabel('epoch')\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "ex1_scores = ex1_model.evaluate(X_test, Y_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (ex1_model.metrics_names[1], ex1_scores[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Tune training parameters\n",
    "\n",
    "Try to improve the classification accuracy, in particular by trying different optimizers and playing with the parameters of the training process.\n",
    "\n",
    "See optimizers available in Keras here: <https://keras.io/api/optimizers/#available-optimizers>\n",
    "\n",
    "The parameters of the `fit()` method are discussed here: <https://keras.io/api/models/model_training_apis/#fit-method>\n",
    "\n",
    "You can take the model created in Task 1 as a starting point. Below is a code example which you can modify.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 - 21s - loss: 0.2792 - accuracy: 0.9161 - 21s/epoch - 44ms/step\n",
      "Epoch 2/10\n",
      "469/469 - 19s - loss: 0.1009 - accuracy: 0.9701 - 19s/epoch - 42ms/step\n",
      "Epoch 3/10\n",
      "469/469 - 20s - loss: 0.0720 - accuracy: 0.9779 - 20s/epoch - 42ms/step\n",
      "Epoch 4/10\n",
      "469/469 - 20s - loss: 0.0610 - accuracy: 0.9814 - 20s/epoch - 42ms/step\n",
      "Epoch 5/10\n",
      "469/469 - 20s - loss: 0.0522 - accuracy: 0.9841 - 20s/epoch - 42ms/step\n",
      "Epoch 6/10\n",
      "469/469 - 20s - loss: 0.0465 - accuracy: 0.9857 - 20s/epoch - 42ms/step\n",
      "Epoch 7/10\n",
      "469/469 - 20s - loss: 0.0401 - accuracy: 0.9871 - 20s/epoch - 42ms/step\n",
      "Epoch 8/10\n",
      "469/469 - 20s - loss: 0.0385 - accuracy: 0.9878 - 20s/epoch - 42ms/step\n",
      "Epoch 9/10\n",
      "469/469 - 19s - loss: 0.0350 - accuracy: 0.9891 - 19s/epoch - 41ms/step\n",
      "Epoch 10/10\n",
      "469/469 - 20s - loss: 0.0320 - accuracy: 0.9898 - 20s/epoch - 43ms/step\n",
      "313/313 - 2s - loss: 0.0292 - accuracy: 0.9906 - 2s/epoch - 7ms/step\n",
      "accuracy: 99.06%\n"
     ]
    }
   ],
   "source": [
    "# Clone the model from Task 1\n",
    "ex2_model = keras.models.clone_model(ex1_model)\n",
    "\n",
    "# Compile the model with a different optimizer and learning rate\n",
    "ex2_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model for more epochs\n",
    "epochs = 10\n",
    "ex2_history = ex2_model.fit(X_train, Y_train, epochs=epochs, batch_size=128, verbose=2)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "ex2_scores = ex2_model.evaluate(X_test, Y_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (ex2_model.metrics_names[1], ex2_scores[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*Run this notebook in Google Colaboratory using [this link](https://colab.research.google.com/github/csc-training/intro-to-dl/blob/master/day1/03-tf2-mnist-cnn.ipynb).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
